{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import getcwd, path\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "BASE_PATH = path.dirname(getcwd())\n",
    "sys.path.append(BASE_PATH)\n",
    "\n",
    "from text_classification.convnet.train import trainIters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/2359media/anaconda/envs/botbot-nlp/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(path=\"imdb.npz\",\n",
    "                                                      num_words=None,\n",
    "                                                      skip_top=0,\n",
    "                                                      maxlen=None,\n",
    "                                                      seed=113,\n",
    "                                                      start_char=1,\n",
    "                                                      oov_char=2,\n",
    "                                                      index_from=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform the data back to plain text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_id = imdb.get_word_index()\n",
    "word_to_id = {k:(v+3) for k,v in word_to_id.items()}\n",
    "word_to_id[\"<PAD>\"] = 0\n",
    "word_to_id[\"<START>\"] = 1\n",
    "word_to_id[\"<UNK>\"] = 2\n",
    "id_to_word = {value:key for key,value in word_to_id.items()}\n",
    "# We actually don't need these tokens\n",
    "id_to_word[0] = \"\"\n",
    "id_to_word[1] = \"\"\n",
    "id_to_word[2] = \"\"\n",
    "\n",
    "def inverse_transform(seq):\n",
    "    try:\n",
    "        return ' '.join(id_to_word[id] for id in seq).strip()\n",
    "    except:\n",
    "        print(seq)\n",
    "        return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_text = [inverse_transform(item) for item in x_train]\n",
    "x_test_text  = [inverse_transform(item) for item in x_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing /Users/2359media/Documents/botbot-nlp/data/fasttext/crawl-300d-2M.vec...\n"
     ]
    }
   ],
   "source": [
    "training_data = [(item, y_train[idx]) for idx, item in enumerate(x_train_text)]\n",
    "classes = ['positive', 'negative']\n",
    "\n",
    "losses, model = trainIters(training_data, \n",
    "                           classes, \n",
    "                           n_iters=10, \n",
    "                           log_every=1, \n",
    "                           verbose=1,\n",
    "                           learning_rate=1e-3, \n",
    "                           batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "plt.figure()\n",
    "fig, ax = plt.subplots()\n",
    "loc = ticker.MultipleLocator(base=5)\n",
    "ax.yaxis.set_major_locator(loc)\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_classification.convnet.train import evaluate_all\n",
    "test_data = [(item, y_test[idx]) for idx, item in enumerate(x_test_text)]\n",
    "print(evaluate_all(model, test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from common.utils import argmax, to_scalar\n",
    "from text_classification.fast_text.model import process_sentences\n",
    "\n",
    "TEST_SENTENCE = x_train_text[5]\n",
    "print(TEST_SENTENCE)\n",
    "\n",
    "result = model(*process_sentences([TEST_SENTENCE]))\n",
    "result = F.softmax(result, dim=1)\n",
    "max_idx = argmax(result)\n",
    "print((classes[max_idx], to_scalar(result[0][max_idx])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
