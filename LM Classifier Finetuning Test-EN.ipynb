{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import getcwd, path, environ\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "BASE_PATH = getcwd()\n",
    "sys.path.append(BASE_PATH)\n",
    "\n",
    "environ['NUM_WORKERS'] = '0'\n",
    "# environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luungoc2005/miniconda3/envs/nlp/lib/python3.7/site-packages/ipykernel_launcher.py:4: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  after removing the cwd from sys.path.\n",
      "/home/luungoc2005/miniconda3/envs/nlp/lib/python3.7/site-packages/ipykernel_launcher.py:9: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>57447.0</td>\n",
       "      <td>1</td>\n",
       "      <td>This was on at 2 or so In the morning one Satu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>42373.0</td>\n",
       "      <td>0</td>\n",
       "      <td>JUDAAI was a bold film by Raj Kanwar at it's t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1158910.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Man, I loved this movie! This really takes me ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8571.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Many King fans hate this because it departed f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>120003.0</td>\n",
       "      <td>0</td>\n",
       "      <td>The only possible way to enjoy this flick is t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  label                                           sentence\n",
       "0    57447.0      1  This was on at 2 or so In the morning one Satu...\n",
       "1    42373.0      0  JUDAAI was a bold film by Raj Kanwar at it's t...\n",
       "2  1158910.0      1  Man, I loved this movie! This really takes me ...\n",
       "3     8571.0      0  Many King fans hate this because it departed f...\n",
       "4   120003.0      0  The only possible way to enjoy this flick is t..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_imdb = pd.read_json('data/imdb/train.json')\n",
    "data_binary_sst = pd.read_csv('data/binary_sst/train.csv')\n",
    "# data_semeval = pd.read_csv('data/semeval/train.csv')\n",
    "data = pd.concat([data_imdb, data_binary_sst], ignore_index=True)\n",
    "\n",
    "val_data_imdb = pd.read_json('data/imdb/test.json')\n",
    "val_data_binary_sst = pd.read_csv('data/binary_sst/test.csv')\n",
    "# data_semeval = pd.read_csv('data/semeval/train.csv')\n",
    "val_data = pd.concat([val_data_imdb, val_data_binary_sst], ignore_index=True)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>57447.0</td>\n",
       "      <td>positive</td>\n",
       "      <td>This was on at 2 or so In the morning one Satu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>42373.0</td>\n",
       "      <td>negative</td>\n",
       "      <td>JUDAAI was a bold film by Raj Kanwar at it's t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1158910.0</td>\n",
       "      <td>positive</td>\n",
       "      <td>Man, I loved this movie! This really takes me ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8571.0</td>\n",
       "      <td>negative</td>\n",
       "      <td>Many King fans hate this because it departed f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>120003.0</td>\n",
       "      <td>negative</td>\n",
       "      <td>The only possible way to enjoy this flick is t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id     label                                           sentence\n",
       "0    57447.0  positive  This was on at 2 or so In the morning one Satu...\n",
       "1    42373.0  negative  JUDAAI was a bold film by Raj Kanwar at it's t...\n",
       "2  1158910.0  positive  Man, I loved this movie! This really takes me ...\n",
       "3     8571.0  negative  Many King fans hate this because it departed f...\n",
       "4   120003.0  negative  The only possible way to enjoy this flick is t..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[data['label'] == 0, 'label'] = 'negative'\n",
    "data.loc[data['label'] == 1, 'label'] = 'positive'\n",
    "\n",
    "val_data.loc[val_data['label'] == 0, 'label'] = 'negative'\n",
    "val_data.loc[val_data['label'] == 1, 'label'] = 'positive'\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['sentence'] = data['sentence'].str.replace('\\n', ' ').replace('<br />', ' ').replace('  ', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default language for this instance: en\n"
     ]
    }
   ],
   "source": [
    "from sent_to_vec.masked_lm.bert_model import BertLMWrapper\n",
    "from sent_to_vec.masked_lm.train import LanguageModelLearner\n",
    "from sent_to_vec.masked_lm.data import WikiTextDataset\n",
    "\n",
    "from common.modules import BertAdam\n",
    "from common.callbacks import PrintLoggerCallback, EarlyStoppingCallback, ModelCheckpointCallback, TensorboardCallback, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Featurizer previously fitted, continuing\n",
      "Found 1732377 tokens\n"
     ]
    }
   ],
   "source": [
    "model = BertLMWrapper(from_fp='bert_en_base.bin')\n",
    "model.init_model()\n",
    "\n",
    "dataset = WikiTextDataset()\n",
    "dataset.initialize(model, data_texts=list(data['sentence']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training in BERT mode\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 16\n",
    "n_epochs=5\n",
    "\n",
    "learner = LanguageModelLearner(model,\n",
    "    optimizer_fn=BertAdam,\n",
    "    optimizer_kwargs={\n",
    "        'lr': 2e-5,\n",
    "        't_total': n_epochs * (len(dataset) // BATCH_SIZE),\n",
    "        'warmup': 0.04\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient accumulation is supported by this class\n",
      "Number of tokens 36000\n",
      "BertForMaskedLM(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(36000, 576, padding_idx=0)\n",
      "      (position_embeddings): Embedding(104, 576, padding_idx=0)\n",
      "      (token_type_embeddings): Embedding(2, 576, padding_idx=0)\n",
      "      (LayerNorm): FusedLayerNorm(torch.Size([576]), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.15)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=576, out_features=576, bias=True)\n",
      "              (key): Linear(in_features=576, out_features=576, bias=True)\n",
      "              (value): Linear(in_features=576, out_features=576, bias=True)\n",
      "              (dropout): Dropout(p=0.15)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=576, out_features=576, bias=True)\n",
      "              (LayerNorm): FusedLayerNorm(torch.Size([576]), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.15)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=576, out_features=1200, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=1200, out_features=576, bias=True)\n",
      "            (LayerNorm): FusedLayerNorm(torch.Size([576]), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.15)\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=576, out_features=576, bias=True)\n",
      "              (key): Linear(in_features=576, out_features=576, bias=True)\n",
      "              (value): Linear(in_features=576, out_features=576, bias=True)\n",
      "              (dropout): Dropout(p=0.15)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=576, out_features=576, bias=True)\n",
      "              (LayerNorm): FusedLayerNorm(torch.Size([576]), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.15)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=576, out_features=1200, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=1200, out_features=576, bias=True)\n",
      "            (LayerNorm): FusedLayerNorm(torch.Size([576]), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.15)\n",
      "          )\n",
      "        )\n",
      "        (2): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=576, out_features=576, bias=True)\n",
      "              (key): Linear(in_features=576, out_features=576, bias=True)\n",
      "              (value): Linear(in_features=576, out_features=576, bias=True)\n",
      "              (dropout): Dropout(p=0.15)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=576, out_features=576, bias=True)\n",
      "              (LayerNorm): FusedLayerNorm(torch.Size([576]), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.15)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=576, out_features=1200, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=1200, out_features=576, bias=True)\n",
      "            (LayerNorm): FusedLayerNorm(torch.Size([576]), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.15)\n",
      "          )\n",
      "        )\n",
      "        (3): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=576, out_features=576, bias=True)\n",
      "              (key): Linear(in_features=576, out_features=576, bias=True)\n",
      "              (value): Linear(in_features=576, out_features=576, bias=True)\n",
      "              (dropout): Dropout(p=0.15)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=576, out_features=576, bias=True)\n",
      "              (LayerNorm): FusedLayerNorm(torch.Size([576]), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.15)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=576, out_features=1200, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=1200, out_features=576, bias=True)\n",
      "            (LayerNorm): FusedLayerNorm(torch.Size([576]), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.15)\n",
      "          )\n",
      "        )\n",
      "        (4): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=576, out_features=576, bias=True)\n",
      "              (key): Linear(in_features=576, out_features=576, bias=True)\n",
      "              (value): Linear(in_features=576, out_features=576, bias=True)\n",
      "              (dropout): Dropout(p=0.15)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=576, out_features=576, bias=True)\n",
      "              (LayerNorm): FusedLayerNorm(torch.Size([576]), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.15)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=576, out_features=1200, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=1200, out_features=576, bias=True)\n",
      "            (LayerNorm): FusedLayerNorm(torch.Size([576]), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.15)\n",
      "          )\n",
      "        )\n",
      "        (5): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=576, out_features=576, bias=True)\n",
      "              (key): Linear(in_features=576, out_features=576, bias=True)\n",
      "              (value): Linear(in_features=576, out_features=576, bias=True)\n",
      "              (dropout): Dropout(p=0.15)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=576, out_features=576, bias=True)\n",
      "              (LayerNorm): FusedLayerNorm(torch.Size([576]), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.15)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=576, out_features=1200, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=1200, out_features=576, bias=True)\n",
      "            (LayerNorm): FusedLayerNorm(torch.Size([576]), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.15)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=576, out_features=576, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (cls): BertOnlyMLMHead(\n",
      "    (predictions): BertLMPredictionHead(\n",
      "      (transform): BertPredictionHeadTransform(\n",
      "        (dense): Linear(in_features=576, out_features=576, bias=True)\n",
      "        (LayerNorm): FusedLayerNorm(torch.Size([576]), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "      (decoder): Linear(in_features=576, out_features=36000, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (adasoft): SplitCrossEntropyLoss()\n",
      ")\n",
      "1m 53s (- 7m 34s) (1 20%) - loss: 0.3294\n",
      "4m 12s (- 6m 18s) (2 40%) - loss: 0.3131\n",
      "6m 31s (- 4m 20s) (3 60%) - loss: 0.3060\n",
      "8m 49s (- 2m 12s) (4 80%) - loss: 0.3020\n",
      "11m 9s (- 0m 0s) (5 100%) - loss: 0.2992\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "learner.fit(\n",
    "    training_data=dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=n_epochs,\n",
    "    callbacks=[\n",
    "        PrintLoggerCallback(log_every=1, metrics=['loss']),\n",
    "    ],\n",
    "    gradient_accumulation_steps=10,\n",
    "    clip_grad=1.0,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('bert_vi_sentiment_lm.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_classification.with_pretrained.model import LMClassifierWrapper\n",
    "from text_classification.with_pretrained.train import LMClassifierLearner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs=30\n",
    "classifier = LMClassifierWrapper(encoder=model)\n",
    "classifier_learner = LMClassifierLearner(\n",
    "    classifier,\n",
    "    optimizer_fn=BertAdam,\n",
    "    optimizer_kwargs={\n",
    "        'lr': 2e-5,\n",
    "        't_total': n_epochs * (len(dataset) // BATCH_SIZE),\n",
    "        'warmup': 0.04\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier.model.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_words': 36000, 'hidden_size': 576, 'num_hidden_layers': 6, 'num_attention_heads': 12, 'intermediate_size': 1200, 'hidden_act': 'gelu', 'hidden_dropout_prob': 0.15, 'attention_probs_dropout_prob': 0.15, 'max_position_embeddings': 104, 'featurizer_seq_len': 104, 'type_vocab_size': 2, 'initializer_range': 0.025, 'use_adasoft': True, 'append_sos_eos': True, 'featurizer_reserved_tokens': ['<START>', '<STOP>', '<UNK>', '<MASK>'], 'tokenize_fn': <function word_tokenize at 0x7f7b8798fc80>, 'input_shape': (None,), 'adasoft_cutoffs': [2000, 4000, 10000]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/luungoc2005/Data/Projects/botbot-nlp/common/wrappers.py:582: UserWarning: Error orcurred in multiprocessing.set_start_method\n",
      "  warnings.warn('Error orcurred in multiprocessing.set_start_method')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3m 31s (- 102m 26s) (1 3%) - loss: 0.6204 - accuracy: 0.6740\n",
      "{'accuracy': tensor(0.6875, device='cuda:0'), 'loss': 0.5257763266563416}\n",
      "1995\n",
      "TEST:  - val_accuracy: 0.0002\n",
      "Model Checkpoint: Saving checkpoint: _ 3m 31s _epoch 1_1995 loss_0_6204 accuracy_0_6740.bin\n"
     ]
    }
   ],
   "source": [
    "classifier_learner.fit(\n",
    "    training_data=(data['sentence'], data['label']),\n",
    "    validation_data=(val_data['sentence'], val_data['label']),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=n_epochs, \n",
    "    callbacks=[\n",
    "        PrintLoggerCallback(log_every=1),\n",
    "        ModelCheckpointCallback()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_imdb = pd.read_json('data/imdb/test.json')\n",
    "test_data_binary_sst = pd.read_csv('data/binary_sst/test.csv')\n",
    "# data_semeval = pd.read_csv('data/semeval/train.csv')\n",
    "test_data = pd.concat([test_data_imdb, test_data_binary_sst], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "test_item = [test_data['sentence'][5]]\n",
    "classifier(test_item, return_logits=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "classifier.save('bert_en_sentiment.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_model = classifier.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.label_encoder.classes_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier(test_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from common.torch_utils import to_gpu\n",
    "\n",
    "# _, seq_tokens = raw_model.encoder(\n",
    "#     to_gpu(classifier.featurizer.transform(test_item))\n",
    "# )\n",
    "# sequence_output = raw_model.rnn(seq_tokens)[0]\n",
    "# print(sequence_output)\n",
    "\n",
    "# output, idxs = torch.max(sequence_output, 0)\n",
    "# print(idxs)\n",
    "# idxs = idxs.data.cpu().numpy()\n",
    "\n",
    "# sent = classifier.featurizer.transform(test_item)\n",
    "# raw_sent = classifier.featurizer.inverse_transform(sent)\n",
    "# print(raw_sent)\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# argmaxs = [np.sum((idxs == k)) for k in range(len(sent[0]))]\n",
    "# # argmaxs[0] = 1e-8\n",
    "# print(argmaxs)\n",
    "# x = range(len(sent[0]))\n",
    "# y = [100.0 * n / np.sum(argmaxs) for n in argmaxs]\n",
    "# print(y)\n",
    "\n",
    "# plt.xticks(x, raw_sent[0], rotation=45)\n",
    "# plt.bar(x, y)\n",
    "# plt.ylabel('%')\n",
    "# plt.title('Visualisation of words importance')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = LMClassifierWrapper(from_fp='bert_en_sentiment.bin')\n",
    "loaded_model.init_model()\n",
    "\n",
    "print(loaded_model(test_item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier(test_item, return_logits=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loaded_model.__getstate__()['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classifier.__getstate__()['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(model_1, model_2):\n",
    "    models_differ = 0\n",
    "    for key_item_1, key_item_2 in zip(model_1.state_dict().items(), model_2.state_dict().items()):\n",
    "        if torch.equal(key_item_1[1], key_item_2[1]):\n",
    "            pass\n",
    "        else:\n",
    "            models_differ += 1\n",
    "            if (key_item_1[0] == key_item_2[0]):\n",
    "                print('Mismtach found at', key_item_1[0])\n",
    "            else:\n",
    "                raise Exception\n",
    "    if models_differ == 0:\n",
    "        print('Models match perfectly! :)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_models(loaded_model.model, classifier.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.featurizer.transform(test_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = classifier.featurizer.transform(test_item).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_loaded_model = loaded_model.model\n",
    "raw_classifier = classifier.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_loaded_model.eval()\n",
    "raw_classifier.eval()\n",
    "raw_loaded_model.encoder = raw_classifier.encoder\n",
    "raw_loaded_model(X_train)[0] - raw_classifier(X_train)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.torch_utils import cauchy\n",
    "raw_loaded_model.eval()\n",
    "raw_classifier.eval()\n",
    "\n",
    "encoded = raw_loaded_model.encoder(X_train)[1]\n",
    "rnn = raw_loaded_model.rnn(encoded)[0]\n",
    "max_pool = torch.max(rnn, 1)[0]\n",
    "pooled = max_pool\n",
    "prev_pooled = max_pool\n",
    "# print(prev_pooled - pooled)\n",
    "# pooled = raw_loaded_model.pooler(max_pool)\n",
    "for ix, layer in enumerate(raw_loaded_model.pooler):\n",
    "    pooled = layer(pooled)\n",
    "    prev_pooled = raw_classifier.pooler[ix](prev_pooled)\n",
    "#     print(ix)\n",
    "#     print(prev_pooled - pooled)\n",
    "#     print(layer.weight - raw_classifier.pooler[ix].weight)\n",
    "#     print(layer.bias - raw_classifier.pooler[ix].bias)\n",
    "#     print(ix)\n",
    "#     print(len(raw_loaded_model.pooler))\n",
    "#     print(pooled - prev_pooled)\n",
    "    if ix < len(raw_loaded_model.pooler) - 1:\n",
    "        pooled = torch.nn.functional.relu(pooled)\n",
    "        prev_pooled = torch.nn.functional.relu(prev_pooled)\n",
    "#         print(prev_pooled - pooled)\n",
    "#         print(pooled)\n",
    "    else:\n",
    "        pooled = cauchy(pooled)\n",
    "        prev_pooled = cauchy(prev_pooled)\n",
    "#         print(prev_pooled - pooled)\n",
    "#         print(pooled)\n",
    "\n",
    "# print(pooled - prev_pooled)\n",
    "print(raw_loaded_model.classifier.bias)\n",
    "print(raw_classifier.classifier.bias)\n",
    "with torch.no_grad():\n",
    "    print(raw_loaded_model.classifier.weight - raw_classifier.classifier.weight)\n",
    "    print(raw_loaded_model.classifier(prev_pooled))\n",
    "    print(raw_classifier.classifier(prev_pooled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loaded_model.model.encoder == raw_loaded_model.encoder)\n",
    "print(loaded_model(test_item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
