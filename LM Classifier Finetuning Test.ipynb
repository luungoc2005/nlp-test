{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import getcwd, path, environ\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "BASE_PATH = getcwd()\n",
    "sys.path.append(BASE_PATH)\n",
    "\n",
    "environ['NUM_WORKERS'] = '0'\n",
    "# environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_000000</td>\n",
       "      <td>Dung dc sp tot cam on  shop Đóng gói sản phẩm ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_000001</td>\n",
       "      <td>Chất lượng sản phẩm tuyệt vời . Son mịn nhưng...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_000002</td>\n",
       "      <td>Chất lượng sản phẩm tuyệt vời nhưng k có hộp ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_000003</td>\n",
       "      <td>:(( Mình hơi thất vọng 1 chút vì mình đã kỳ vọ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_000004</td>\n",
       "      <td>Lần trước mình mua áo gió màu hồng rất ok mà đ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                            comment  label\n",
       "0  train_000000  Dung dc sp tot cam on  shop Đóng gói sản phẩm ...      0\n",
       "1  train_000001   Chất lượng sản phẩm tuyệt vời . Son mịn nhưng...      0\n",
       "2  train_000002   Chất lượng sản phẩm tuyệt vời nhưng k có hộp ...      0\n",
       "3  train_000003  :(( Mình hơi thất vọng 1 chút vì mình đã kỳ vọ...      1\n",
       "4  train_000004  Lần trước mình mua áo gió màu hồng rất ok mà đ...      1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data/vn_sentiment/train.csv')\n",
    "\n",
    "# replace new lines with space\n",
    "data['comment'] = data['comment'].str.replace('\\n', ' ')\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default language for this instance: en\n"
     ]
    }
   ],
   "source": [
    "from sent_to_vec.masked_lm.bert_model import BertLMWrapper\n",
    "from sent_to_vec.masked_lm.train import LanguageModelLearner\n",
    "from sent_to_vec.masked_lm.vi_data import ViTextDataset\n",
    "\n",
    "from common.modules import BertAdam\n",
    "from common.callbacks import PrintLoggerCallback, EarlyStoppingCallback, ModelCheckpointCallback, TensorboardCallback, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Featurizer previously fitted, continuing\n",
      "Found 1321308 tokens\n",
      "Top 5 words: <START>, <STOP>, <UNK>, <MASK>, ,\n"
     ]
    }
   ],
   "source": [
    "model = BertLMWrapper(from_fp='bert_vi_base.bin')\n",
    "model.init_model()\n",
    "\n",
    "dataset = ViTextDataset()\n",
    "dataset.initialize(model, data_texts=list(data['comment']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training in BERT mode\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 16\n",
    "n_epochs=10\n",
    "\n",
    "learner = LanguageModelLearner(model,\n",
    "    optimizer_fn=BertAdam,\n",
    "    optimizer_kwargs={\n",
    "        'lr': 3e-5,\n",
    "        't_total': n_epochs * (len(dataset) // BATCH_SIZE),\n",
    "        'warmup': 0.04\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient accumulation is supported by this class\n",
      "Number of tokens 30000\n",
      "BertForMaskedLM(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30000, 512, padding_idx=0)\n",
      "      (position_embeddings): Embedding(100, 512, padding_idx=0)\n",
      "      (token_type_embeddings): Embedding(2, 512, padding_idx=0)\n",
      "      (LayerNorm): FusedLayerNorm(torch.Size([512]), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (key): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (value): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (LayerNorm): FusedLayerNorm(torch.Size([512]), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=512, out_features=1140, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=1140, out_features=512, bias=True)\n",
      "            (LayerNorm): FusedLayerNorm(torch.Size([512]), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (key): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (value): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (LayerNorm): FusedLayerNorm(torch.Size([512]), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=512, out_features=1140, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=1140, out_features=512, bias=True)\n",
      "            (LayerNorm): FusedLayerNorm(torch.Size([512]), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (2): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (key): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (value): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (LayerNorm): FusedLayerNorm(torch.Size([512]), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=512, out_features=1140, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=1140, out_features=512, bias=True)\n",
      "            (LayerNorm): FusedLayerNorm(torch.Size([512]), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (3): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (key): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (value): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (LayerNorm): FusedLayerNorm(torch.Size([512]), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=512, out_features=1140, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=1140, out_features=512, bias=True)\n",
      "            (LayerNorm): FusedLayerNorm(torch.Size([512]), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (4): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (key): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (value): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (LayerNorm): FusedLayerNorm(torch.Size([512]), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=512, out_features=1140, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=1140, out_features=512, bias=True)\n",
      "            (LayerNorm): FusedLayerNorm(torch.Size([512]), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (5): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (key): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (value): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (LayerNorm): FusedLayerNorm(torch.Size([512]), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=512, out_features=1140, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=1140, out_features=512, bias=True)\n",
      "            (LayerNorm): FusedLayerNorm(torch.Size([512]), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (cls): BertOnlyMLMHead(\n",
      "    (predictions): BertLMPredictionHead(\n",
      "      (transform): BertPredictionHeadTransform(\n",
      "        (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (LayerNorm): FusedLayerNorm(torch.Size([512]), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "      (decoder): Linear(in_features=512, out_features=30000, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (adasoft): SplitCrossEntropyLoss()\n",
      ")\n",
      "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "0m 45s (- 6m 50s) (1 10%) - loss: 0.4147\n",
      "1m 30s (- 6m 1s) (2 20%) - loss: 0.3585\n",
      "2m 13s (- 5m 10s) (3 30%) - loss: 0.3427\n",
      "2m 55s (- 4m 23s) (4 40%) - loss: 0.3312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0\n",
      "3m 38s (- 3m 38s) (5 50%) - loss: 0.3225\n",
      "4m 21s (- 2m 54s) (6 60%) - loss: 0.3165\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0\n",
      "5m 3s (- 2m 10s) (7 70%) - loss: 0.3125\n",
      "5m 46s (- 1m 26s) (8 80%) - loss: 0.3075\n",
      "6m 30s (- 0m 43s) (9 90%) - loss: 0.3053\n",
      "7m 13s (- 0m 0s) (10 100%) - loss: 0.3025\n"
     ]
    }
   ],
   "source": [
    "learner.fit(\n",
    "    training_data=dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=n_epochs,\n",
    "    callbacks=[\n",
    "        PrintLoggerCallback(log_every=1, metrics=['loss']),\n",
    "    ],\n",
    "    gradient_accumulation_steps=10,\n",
    "    clip_grad=1.0,\n",
    "    fp16=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('bert_vi_sentiment_lm.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_classification.with_pretrained.model import LMClassifierWrapper\n",
    "from text_classification.with_pretrained.train import LMClassifierLearner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = LMClassifierWrapper({\n",
    "    'encoder': model\n",
    "})\n",
    "classifier_learner = LMClassifierLearner(\n",
    "    classifier,\n",
    "    optimizer_fn='adam'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/luungoc2005/Data/Projects/botbot-nlp/common/wrappers.py:571: UserWarning: Error orcurred in multiprocessing.set_start_method\n",
      "  warnings.warn('Error orcurred in multiprocessing.set_start_method')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4m 19s (- 38m 54s) (5 10%) - loss: 0.1668 - accuracy: 0.9558\n",
      "8m 36s (- 34m 27s) (10 20%) - loss: 0.1133 - accuracy: 0.9782\n",
      "12m 53s (- 30m 4s) (15 30%) - loss: 0.0951 - accuracy: 0.9835\n",
      "17m 10s (- 25m 45s) (20 40%) - loss: 0.0842 - accuracy: 0.9873\n",
      "21m 27s (- 21m 27s) (25 50%) - loss: 0.0798 - accuracy: 0.9889\n",
      "25m 44s (- 17m 9s) (30 60%) - loss: 0.0819 - accuracy: 0.9874\n",
      "30m 1s (- 12m 51s) (35 70%) - loss: 0.0760 - accuracy: 0.9897\n",
      "34m 18s (- 8m 34s) (40 80%) - loss: 0.0746 - accuracy: 0.9902\n",
      "38m 35s (- 4m 17s) (45 90%) - loss: 0.0725 - accuracy: 0.9909\n",
      "42m 53s (- 0m 0s) (50 100%) - loss: 0.0671 - accuracy: 0.9925\n"
     ]
    }
   ],
   "source": [
    "classifier_learner.fit(\n",
    "    training_data=(data['comment'], data['label']), \n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=50, \n",
    "    callbacks=[PrintLoggerCallback(log_every=5)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('data/vn_sentiment/test.csv')\n",
    "\n",
    "# replace new lines with space\n",
    "test_data['comment'] = test_data['comment'].str.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_000000</td>\n",
       "      <td>Chưa dùng thử nên chưa biết</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_000001</td>\n",
       "      <td>Không đáng tiềnVì ngay đợt sale nên mới mua n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_000002</td>\n",
       "      <td>Cám ơn shop. Đóng gói sản phẩm rất đẹp và chắc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test_000003</td>\n",
       "      <td>Vải đẹp.phom oki luôn.quá ưng</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test_000004</td>\n",
       "      <td>Chuẩn hàng đóng gói đẹp</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                            comment\n",
       "0  test_000000                        Chưa dùng thử nên chưa biết\n",
       "1  test_000001   Không đáng tiềnVì ngay đợt sale nên mới mua n...\n",
       "2  test_000002  Cám ơn shop. Đóng gói sản phẩm rất đẹp và chắc...\n",
       "3  test_000003                      Vải đẹp.phom oki luôn.quá ưng\n",
       "4  test_000004                            Chuẩn hàng đóng gói đẹp"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9596, 0.0133, 0.0271]], device='cuda:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "test_item = [test_data['comment'][5]]\n",
    "torch.softmax(classifier(test_item, return_logits=True)[0], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/luungoc2005/Data/Projects/botbot-nlp/common/wrappers.py:150: UserWarning: get_state_dict() is not implemented. Using default implementation\n",
      "  warnings.warn('get_state_dict() is not implemented. Using default implementation')\n"
     ]
    }
   ],
   "source": [
    "classifier.save('bert_vi_sentiment.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_model = classifier.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.label_encoder.classes_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "y contains previously unseen labels: [2]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-82e53a216832>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_item\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/media/luungoc2005/Data/Projects/botbot-nlp/common/wrappers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__str__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/luungoc2005/Data/Projects/botbot-nlp/common/wrappers.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X, interpret_fn, return_logits, *args, **kwargs)\u001b[0m\n\u001b[1;32m    229\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0minterpret_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfreeze_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/luungoc2005/Data/Projects/botbot-nlp/text_classification/with_pretrained/model.py\u001b[0m in \u001b[0;36minfer_predict\u001b[0;34m(self, logits, topk)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minfer_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0minfer_classification_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/media/luungoc2005/Data/Projects/botbot-nlp/text_classification/utils/inference.py\u001b[0m in \u001b[0;36minfer_classification_output\u001b[0;34m(model, logits, topk, contexts)\u001b[0m\n\u001b[1;32m     67\u001b[0m     top_classes = [\n\u001b[1;32m     68\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_idxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m     ]\n\u001b[1;32m     71\u001b[0m     return [\n",
      "\u001b[0;32m/media/luungoc2005/Data/Projects/botbot-nlp/text_classification/utils/inference.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     67\u001b[0m     top_classes = [\n\u001b[1;32m     68\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_idxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m     ]\n\u001b[1;32m     71\u001b[0m     return [\n",
      "\u001b[0;32m~/miniconda3/envs/nlp/lib/python3.7/site-packages/sklearn/preprocessing/label.py\u001b[0m in \u001b[0;36minverse_transform\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m             raise ValueError(\n\u001b[0;32m--> 281\u001b[0;31m                     \"y contains previously unseen labels: %s\" % str(diff))\n\u001b[0m\u001b[1;32m    282\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: y contains previously unseen labels: [2]"
     ]
    }
   ],
   "source": [
    "classifier(test_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, seq_tokens = raw_model.encoder(test_item)\n",
    "sequence_output = raw_model.rnn(seq_tokens)[0]\n",
    "print(sequence_output)\n",
    "\n",
    "output, idxs = torch.max(sequence_output, 0)\n",
    "print(idxs)\n",
    "idxs = idxs.data.cpu().numpy()\n",
    "\n",
    "sent = raw_model.encoder.featurizer.transform(test_item)\n",
    "raw_sent = raw_model.encoder.featurizer.inverse_transform(sent)\n",
    "print(raw_sent)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "argmaxs = [np.sum((idxs == k)) for k in range(len(sent[0]))]\n",
    "# argmaxs[0] = 1e-8\n",
    "print(argmaxs)\n",
    "x = range(len(sent[0]))\n",
    "y = [100.0 * n / np.sum(argmaxs) for n in argmaxs]\n",
    "print(y)\n",
    "\n",
    "plt.xticks(x, raw_sent[0], rotation=45)\n",
    "plt.bar(x, y)\n",
    "plt.ylabel('%')\n",
    "plt.title('Visualisation of words importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = LMClassifierWrapper(from_fp='bert_vi_sentiment.bin')\n",
    "loaded_model.init_model(update_configs={'encoder': model})\n",
    "\n",
    "print(loaded_model(test_item, return_logits=True)[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
